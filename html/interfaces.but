\cfg{xhtml-leaf-level}{0}
\cfg{chapter}{Section}
\cfg{text-title-align}{left}
\cfg{text-indent}{0}
\cfg{text-chapter-numeric}{yes}
\cfg{text-chapter-suffix}{. }
\cfg{text-chapter-underline}{-}
\cfg{text-section-numeric}{0}{yes}
\cfg{text-section-suffix}{0}{. }
\cfg{text-section-underline}{0}{-}
\cfg{xhtml-chapter-numeric}{yes}
\cfg{xhtml-chapter-suffix}{. }
\cfg{xhtml-section-numeric}{0}{yes}
\cfg{xhtml-section-suffix}{0}{. }
\cfg{xhtml-section-numeric}{1}{yes}
\cfg{xhtml-section-suffix}{1}{. }

\cfg{xhtml-address-start}{Copyright &copy; 2002 Simon Tatham.
<br>This document is
<a href="http://www.opencontent.org/">OpenContent</a>.
<br>You may copy and use the text under the terms of the
<a href="http://www.opencontent.org/opl.shtml">OpenContent
Licence</a>.
<br>Please send comments and criticism on this article to
<a href="mailto:anakin@pobox.com">anakin@pobox.com</a>.}

\title Guidelines for Software-to-Software Interface Design

\preamble by \W{http://pobox.com/~anakin/}{Simon Tatham},
professional and free-software programmer

\C{intro} Introduction

Interfaces between pieces of software are ubiquitous in programming.
Most large programs are composed of reasonably well-separated
pieces, with well-defined function call interfaces in place between
those pieces; and many pieces of software communicate with one
another using well-defined network protocols.

Interface and protocol design is a fundamental programming technique
for making the complexity of a large project manageable, and also
for allowing diverse programs from widely differing backgrounds to
communicate usefully. They can help in debugging, help in management
of teams of programmers, help flexibility of software by making
parts interchangeable, and much more. So with all these advantages,
it's no wonder there are a lot of interfaces about: within
individual programs, between programs and their supporting
libraries, between cooperating programs, on the same computer or
across a network. Interfaces are everywhere.

They're not easy to design well, however. There are a lot of
subtleties involved in interface design which may well only become
obvious once your interface is in widespread use, and once it's
difficult and costly to try to change it. In this article I try to
address some of the common interface design issues, both in network
protocols and in function-call interfaces inside a program.

Everything I say in this article is a \e{guideline}. Nothing stated
here is an absolute rule. If you have a good reason to ignore any or
all of the guidelines in this document, you should feel free to go
ahead and do so. But it's better to have considered a design
criterion and decided consciously that you don't need or want to
meet it, than to finish coding and then realise you hadn't even
thought of it in the first place.

\C{whether} Is an interface necessary?

The first question of interface design is so obvious it's easy to
miss: should you separate a program (or part of a program) into two
or more pieces with interfaces between them, or should you just
leave it all as one lump? (This question doesn't really apply to
network protocols, of course: if you have two separate programs that need to
communicate, you have a network protocol almost by definition.)

Certainly, if you are ever going to need an interface, you should
try to have it in place from the start. It costs a lot of time and
effort to put an interface into existing code if there wasn't
already one there. You have to go through the code and carefully
separate it out, often moving functionality long distances to make
sure it ends up on the right side of the interface. Sometimes,
depending on why you're needing to put the interface in, you might
find that a simple piece of computation needs to be divided down the
middle, which is really inconvenient.

On the other hand, designing an interface properly could well take
significant time and effort in itself. So while you certainly don't
want to discover three years later that you should have created an
interface and didn't, you also don't really want to spend lots of
time and effort on an interface you'll never need.

Sometimes the decision is easy. For example, if your program needs
to contain interchangeable parts, you are almost guaranteed to need
a well-defined interface between the main program and those parts. A
document formatter which can output in several formats, for example,
or a network communications program able to speak several different
network protocols, both \e{need} a clear interface to dictate how
the rest of the program talks to any of the interchangeable
sections. There's no other practical way to be sure that swapping
one back end for another one will continue to work sensibly.

Alternatively, suppose a particular part of the program is not
particularly related to the program's main function: for example, in
a network utility, you might have a set of functions which deal with
gradually collecting data and accumulating it into a string. This
might be a good piece of code to define a clear interface to,
because that would make it easier to copy the whole set of functions
out of that program and re-use it in another one. (You could see
this as the interchangeability criterion in reverse: instead of
leaving the main program fixed and switching around small sections
of code, you're leaving a small section of code fixed and switching
the rest of the program!)

Apart from these clear practical advantages of defining an
interface, the main benefit of splitting code into small sections
and defining interfaces between them is in debugging and testing.
Many bugs can be localised into a particular section simply by
examining the data flowing over the interfaces between sections, and
checking whether that data is legal according to the interface
definitions. When you find a section taking in valid data and giving
out invalid data, there's the bug. Better still, you can write a
test suite for each section based on the interface to it, and test
each statement made in the interface design. If the design says
passing a null pointer to a function is valid, for example, you can
write a test which does pass it a null pointer, and make sure it
doesn't crash. That way you can do a lot of your testing on a small
part of the program at a time, and cut down the number of bugs you
have to hunt for in the program as a whole.

Ultimately, though, the decision about whether to define an
interface is one of cost versus benefit. Carefully specifying an
interface takes time and effort (particularly if you carefully
consider all the guidelines in this article!), and it might be that
the time you'll spend doing this is more than the time you'd save in
debugging and testing.

\C{where} Where should I put my interface?

Once you've decided you need an interface, the next big question is
exactly where it goes. Given a piece of functionality that could
plausibly be placed on either side of the interface, how do you
decide which side you put it on?

There are a variety of criteria you might use to decide this
question. In this section I examine some of them.

\H{where-interchangeability} Interchangeability

If the purpose of your interface is to allow several sections of a
program to be interchangeable, this often makes it easy to decide
where a particular piece of functionality belongs. If the
functionality is never going to need to be different depending on
which of the interchangeable parts you're using, it might be good to
put it on the main-program side of the interface and keep your code
maintainable by avoiding duplication. On the other hand, if the
functionality will need to work differently for different back ends,
it should probably be on the back-end side of the interface so that
each back end can supply its own version of it.

These aren't the only options, of course. If the ways in which the
functionality will need to work differently can be easily described
by a few parameters, then another alternative would be to put the
main implementation of the functionality on the main-program side of
the interface, and arrange for each back end to supply a function
the main program can call to retrieve the correct values for those
parameters. Alternatively, if the functionality is always the same
but each back end will need it to be performed at a different stage
of the program, it could be provided in a function defined in the
main program and each back end could call that function when it was
ready.

\H{where-trust} Trust boundaries

If your interface is a network protocol between programs that don't
fully trust one another, then sometimes a piece of functionality
\e{needs} to go on a particular side of an interface to avoid
requiring that trust.

To illustrate this I'm going to use a case study: wildcard handling
in SCP and SFTP. Both of these are file transfer protocols which run
over SSH \#{FIXME: references?}, but they are very different in
design. In particular, if the user asks to retrieve all files from
the server which match a particular wildcard (such as
\q{\c{*.txt}}), then the two protocols handle it in completely
different ways:

\b An SCP client sends the wildcard straight to the server, in
exactly the form the user specified it. The server then searches for
file names that match the wildcard, under whatever wildcard matching
rules make sense for that particular server, and it returns the
names and contents of the files that matched.

\b An SFTP client cannot send the wildcard to the server, because
there is no way to do that in SFTP. Instead, the client must ask the
server for a list of \e{all} the files in the source directory, and
the wildcard matching must be done at the client end. Then, once the
client has decided which file names match the wild card, it sends a
separate request to the server for each individual file.

At first sight, the SCP approach looks more efficient, particularly
in terms of minimising data flow over the network (see
\k{where-dataflow}). After all, if there are ten thousand files in a
directory and only three of them match the wildcard, the SFTP client
will have to retrieve all ten thousand file names before it can
start transferring files!

However, SCP has a fatal flaw: because the wildcard matching is
performed by the server's local wildcard matching rules, the client
doesn't have enough information to know what filenames legitimately
match that wildcard or not. On a Unix system, for example,
\c{[ab]*.dat} would match only file names beginning with \q{a} or
\q{b}; but on a VMS system, \c{[ab]} is a directory specifier, and
so the same wildcard could legitimately match \e{any} file name
ending in \c{.dat}. So if the client sends a request for
\c{[ab]*.dat} and the server sends back a file called
\c{passwords.dat}, the client can't be sure that that wasn't exactly
what the user wanted. In fact a malicious server could send back a
more damaging file name such as \c{.ssh/authorized_keys}, in the
hope that the client would trust it and allow the server to
overwrite a sensitive file.

By doing wildcard matching at the client end and therefore by the
client's choice of rules, SFTP avoids this problem completely;
there's no way the server can attempt to write to a file which the
user hadn't explicitly agreed to. So SFTP would be more suitable for
use on servers offering files for general download, where a lot of
users don't personally know the sysadmin of the server.

Again, these aren't the only options. A third possibility could be
that the protocol could precisely define a set of wildcard matching
rules. Then the server could do the matching in accordance with
those rules, and the client would be able to use the same rules to
validate each filename sent back, to make sure it really did match
the pattern the user had provided. This would have the trust
advantage of SFTP's method, together with the minimum-data-flow
advantage of SCP's method. However, with this approach the user has
even less configurability - not only can the server not choose an
appropriate set of wildcard rules, but now the \e{client} can't
choose a rule set either. Everybody everywhere must use the One True
Ruleset defined in the protocol standard.

\H{where-dataflow} The orange-segment model: minimum data flow

If there are no particularly clear reasons to put an interface in a
particular place, one general guideline that often gives a sensible
way to divide a program into parts is what I call the
\q{orange-segment model}. In this model, the aim is to divide your
program into parts in such a way that the minimum amount of data
flows between the parts.

(I call this the orange-segment model because an orange is something
with a natural way to divide it into pieces. Divide an orange the
right way, and it comes apart cleanly; divide it the wrong way, by
cutting it indiscriminately with a sharp knife, and you will get
covered in juice. The correct way to divide up an orange is in such
a way as to minimise the flow of juice between the pieces.)

For example, suppose you need your program to look a word up in a
dictionary and check that it's a real word; and suppose that you've
decided to put an interface between most of the program and the part
that deals with the dictionary file, so that you can change the file
format easily later. One option might be to have the dictionary
module supply a function \cw{GetNthWord()} which accepts a number N,
and returns the Nth word in the dictionary; then the routine
\cw{DoesWordExist()} which tests for the presence of a particular
word can be on the main-program side of the interface, and work by
calling \cw{GetNthWord()} repeatedly and doing a binary search. But
the orange-segment model suggests that a better way would be for
\cw{DoesWordExist()} to be \e{part of} the interface rather than a
client of it, and for it to be implemented \e{inside} the dictionary
module. That way, less data flows over the interface: instead of ten
or twenty requests each returning a word, there is one single
request which sends a word and receives a boolean response.

This guideline doesn't make sense all the time. In fact, in the
above example, it might easily \e{conflict} with other design
principles: if you needed to deal with dictionaries in a large
number of formats, you would probably want to minimise the amount of
functionality provided by each dictionary driver, to save programmer
time and make it easier to add a new driver. So any routine such as
\cw{DoesWordExist()}, which could be built up from simpler functions
such as \cw{GetNthWord()}, would be considered non-essential and
implemented in the main program.

However, the orange-segment model makes a lot of sense in network
protocols, where the computers at each end are likely to be a lot
faster than the connection between them; so that if you want
efficiency, it's more important to minimise data flow over the
network than to minimise calculation at each end. (See
\k{misc-speed} for more detail on this.) It might also make sense in
a situation where the inside of a particular module dealt with data
in a very different format from the program calling it - so that
every time you call a function in the interface, you incur an
overhead for translating the data between formats.

If neither of these situations apply, the orange-segment model is
probably not what you want to use to make the final decisions about
what goes where in your program. But it often gives a good initial
idea of some rough divisions of the code, and then other design
principles can be used to fill in the details.

\C{what} What to specify in an interface design

Many interface designs are written with important facts missing, and
important points left unspecified. In this section I list a few of
the common things that are left out.

\H{what-allowables} Allowable values for parameters

For a start, make sure you specify the range of allowable parameters
in each request and in each response. If a request includes a pointer
to a data structure, is that pointer allowed to be \cw{NULL}? If it
includes a string, can the string contain absolutely anything, or
are there forbidden characters? If it includes two pointers to data
structures of the same type, are they allowed to be the same
structure (equal pointers)? Can integer arguments be zero or
negative? Can floating-point arguments be zero, negative, infinite
or NaN (Not a Number)? What will happen in each of these cases?

(You don't actually have to be able to handle all these
possibilities; you only have to \e{state} whether or not you can.
It's an acceptable design choice to rule that certain parameter
values are simply \e{illegal}: not in the sense that the function is
required to return an error if you pass them, but in the sense that
you must not pass them in the first place and the function is not
required to handle them correctly. An example in the C library
standard is passing a null pointer to \cw{strlen()}. However, note
that any interface between programs that don't trust each other must
ensure that this type of undefined behaviour doesn't lead to a
security hole: examples are a network protocol, or a system call
interface to an OS kernel.)

\H{what-ordering} Ordering constraints

Another important thing to document is the \e{ordering} constraints
(if any) between requests. Often the implementation of a module
introduces ordering constraints, which don't even occur to the
programmer because they hadn't imagined that anyone might ever try
to violate them. Unfortunately, it's almost always worth assuming
that if enough people use your implementation of an interface,
\e{someone} will try to violate any unwritten requirement in it - so
even if you don't write your code to deal with strange orders of
function calls, at least write the interface specification to make
it clear that you haven't. Then anyone who does that has only
themselves to blame.

Here's a simple example. Suppose you are writing in C or another
non-garbage-collected language, and you have created an interface
composed of several functions:

\b \cw{NewContext()} creates a data structure which the rest of the
interface routines will use to store persistent state.

\b \cw{InitContext()} fills in the structure with some initial data.

\b \cw{ProcessContext()} performs an operation on the context
structure, and returns some result of that operation.

\b \cw{FreeContext()} cleans up a context structure when the user
has finished with it, and frees the allocated memory.

The intended order of calls is basically as shown above. Clients are
expected to call \cw{NewContext()} and then \cw{InitContext()} to
set things up, call \cw{ProcessContext()} once or more during
processing, and eventually clean up by calling \cw{FreeContext()}.
It should be intuitively obvious to any reader that you must not
call \cw{InitContext()} or \cw{ProcessContext()} on a context you've
already called \cw{FreeContext()} on, and that you must not call
\cw{ProcessContext()} on a context before you've called
\cw{InitContext()} on it. This much is simple.

Now, can I call \cw{InitContext()} \e{twice} on the same context? It
might make perfect sense - to avoid the memory management overhead
of freeing one context and creating a new one, I might very well
want to take a context I already have and re-initialise it with some
new data. But if the process of initialising a context involves
allocating memory, this means that \cw{InitContext()} must check
each of the fields it initialises, and if they contain
already-allocated data then it must free it before overwriting it
with new data. Otherwise, calling \cw{InitContext()} twice will
cause a memory leak. So whether or not I'm allowed to re-initialise
a context I've already initialised needs to be documented in the
interface specification. (It's perfectly allowable to decide not to
bother handling this case, as long as the interface design \e{says
so}.)

Also, what if I've called \cw{NewContext()}, but in the process of
gathering data to pass to \cw{InitContext()} I encounter a fatal
error and have to abandon what I'm doing? To clean everything up I
need to call \cw{FreeContext()}. But if the implementation was
written in the assumption that all contexts would be initialised
before being freed, it might (for example) try to iterate through an
array freeing the elements, without first checking that the array
itself was non-\cw{NULL}. In this case freeing an uninitialised
context would cause a crash, so it should be documented in the
interface specification as an illegal order of operations.

A good tool for analysing this sort of thing is to imagine a state
machine. In this example, contexts come in three states
(\q{uninitialised}, \q{initialised} and \q{destroyed}), and it
should be obvious that:

\b \cw{NewContext()} creates a context in state \q{uninitialised};

\b \cw{InitContext()} can be given a context in state
\q{uninitialised}, and will change it to state \q{initialised};

\b \cw{ProcessContext()} is always given a context in state
\q{initialised}, and leaves it in that state;

\b \cw{FreeContext()} can be given a context in state
\q{initialised} and will place it in state \q{destroyed}, after
which it may not be used for anything.

Now if you draw up a table with context states across the top and
function names down the side, and plot the above statements on that
table, you will see something like this:

\c                      uninitialised   initialised    destroyed
\c InitContext()             OK                         NOT OK
\c ProcessContext()        NOT OK           OK          NOT OK
\c FreeContext()                            OK          NOT OK

and you can see that the two cells that are not filled in correspond
exactly to the two questions I raised above: calling
\cw{InitContext} on an already-initialised context, and calling
\cw{FreeContext()} on an uninitialised one. Using this sort of
analysis, you can be reasonably confident that you have considered
all the ordering questions before you publish your interface design.
(You may even want to make this sort of state diagram explicit in
the interface specification itself, so that users can reason about
your interface the same way you did. And in languages such as
Eiffel, you can even make your state diagram explicit in the
\e{language}, so the compiler can check that you aren't calling a
function in an invalid state.)

\H{what-ownership} Ownership of data

Suppose you have provided an interface that writes out a PNG image
file. As well as the image data, PNG images may contain textual data
such as a title and a copyright notice. So your interface might well
contain a function such as \cw{PNGSetTitle()}, to which I can pass a
string containing the title I want my image to have.

I might very well call \cw{PNGSetTitle()} right at the start of my
client routine, and then spend some time putting together the actual
bitmap data to go in the image. Eventually I call
\cw{PNGWriteFile()}, and the file is actually created, written to
disk, and closed. The question is: what happens if I overwrote my
copy of the title string in between the two calls?

\cw{PNGSetTitle()} might perfectly reasonably just store its
argument as a pointer to a string, and expect that pointer still to
point to something reasonable when \cw{PNGWriteFile()} is called. In
this case I need to be careful not to destroy my own copy of the
title string between calling \cw{PNGSetTitle()} and
\cw{PNGWriteFile()}, because my copy of the title string is the only
copy. On the other hand, \cw{PNGSetTitle()} might be expecting this
sort of behaviour, and might take a copy of the title string when
it's called - in which case I can do what I like with my own copy as
soon as \cw{PNGSetTitle()} returns. A good interface design would
specify this one way or the other, so I would know whether I could
safely modify the title string.

This is an example of a data ownership issue. Data ownership issues
are most common in non-garbage-collected languages, because it's
always important to know which part of the code has responsibility
for freeing a particular piece of allocated memory. However, they
can still occur in GC languages: the above example would still be
valid in (say) Java.

In a non-garbage-collected language, there would be a third option
in the above situation: \cw{PNGSetTitle()} could pass ownership of
the title string to the PNG module, in the sense that the PNG module
was then responsible for freeing it. This would mean that it was not
only invalid for me to free the title string before calling
\cw{PNGWriteFile()}: it would be invalid for me to free it even
\e{after} that, because the PNG module would have freed it and I
would cause a crash by freeing it a second time. Also, it would be
forbidden for me to pass a title string that was \e{not} dynamically
allocated, or the PNG module would cause a crash by trying to free
it.

For these reasons, this would be an inconvenient way to define an
interface, and designers would probably not do it. But ownership
issues in the opposite direction, dealing with pieces of data
returned \e{from} interface functions, are much more common.
Consider \cw{asctime()} in the C library, which returns a pointer to
a string, but even after that pointer has been given to the user, it
is still owned by the library, and the user is given constraints on
how long they can expect it to be valid for. An alternative
implementation might have been for the C library designers to rule
that \cw{asctime()} always dynamically allocated its return string;
then the user could be sure it would continue to be valid for as
long as they needed it, but would also have to take responsibility
for ensuring it was eventually freed.

If, like \cw{asctime()}, you do design an interface in which the
user is given a pointer to data you still own, you should make sure
you give the user \e{detailed} information on when they can expect
that data to be valid. And you should make sure it stays valid for
long enough to be useful - \cw{asctime()} in a multi-threaded
program is almost useless, because the buffer it returns could
easily be overwritten by another call to \cw{asctime()} in another
thread before the first thread has even had a chance to read the
data out of it. (See \k{threading} for more on thread-safety.)

\C{bidi} Bidirectional interfaces

A lot of interfaces have a clear client side and a clear server
side. Many function-call interfaces are like this: one module
provides a collection of functions, and another module calls them
when it wants things done. Most network protocols work this way too:
things like POP-3, NNTP, HTTP and so on all contain one party which
makes requests, and another party which responds to them. The server
never makes requests back to the client; occasionally it will return
response codes (such as NNTP's 480 Authentication Required) which
suggest to the client that it might like to try giving some
information to the server before attempting a request again, but
these are more like error returns from a request than requests in
themselves.

There are more complex interfaces in which \e{both} sides need to
make requests of the other side. In this section I describe some
points to watch out for when designing a bidirectional interface.

\H{bidi-callbacks} Preventing uncontrolled recursion

In function-call interfaces, bidirectionality means that each side
of the interface provides some functions which the other side can
call.

Some interfaces of this type are still \e{basically} one-way: the
\q{server} side does make calls to the \q{client} side, but only in
direct response to a request the client made of the server. A good
example of this is \cw{qsort()} in the C library, which is a
function the client program calls when it wants some data sorted. It
just so happens that the easiest way for the client to tell the
library how to decide on what order to sort the data into is to
provide a comparison function which \cw{qsort()} can call back to to
ask the client which way round two items should be sorted; but this
information is conceptually part of the parameters to the
\cw{qsort()} call, and if there had been a sensible way to make it
\e{physically} entirely one of the parameters then it would be.

More general two-way interfaces tend to be found in object-oriented
code; it's quite likely that you might have two (or more) objects,
each of which can call methods of the other. These method calls
between the objects might happen in response to method calls from
outside both objects, or one object might call the other in the
course of responding to a call \e{from} that other object.

In both of these cases, the important thing to ensure is that you
never recurse without limit. If one side of an interface makes a
call to the other, and the other must call back to the first side in
the course of processing that call, then the first side must be very
careful about calling across the interface \e{again}. More
particularly, the interface design must specify exactly how much
recursion of this type is allowable.

An example in an object-oriented environment might be two objects
which each have a method called \cw{HandleEvent()}. A variety of
events (perhaps user-interface events) can take place, and when an
event requires action from an object, the object's
\cw{HandleEvent()} method can be called to take that action.

It might seem perfectly reasonable to implement an object which
responds to a particular event by inventing other events and
dispatching them to other objects' \cw{HandleEvent()} methods. (For
example, one object might be a \q{Select All} button which causes a
whole row of other objects to become selected.) If this happens, the
designer must be careful to ensure that none of the objects which
receive the artificial events respond to them by sending further
events back to the first object; or that, if they do, the
third-level events are not of a type which will cause the recursion
to continue.

\H{bidi-deadlock} Preventing deadlock

In a network protocol, bidirectionality means that each side will be
sending requests to the other. This means that both sides have to be
quite flexible: once a request has been sent, the client (or server)
cannot simply assume that the next piece of data coming back from
the network will be the response. It must be aware that the next
piece of data coming back might be a request from the other party,
and be prepared to respond to that while it is waiting for the
response to its own request.

Similarly to the function-call case described in \k{bidi-callbacks},
each side of the interface must be careful about making a request of
the other side in the process of constructing a response to a
request it has received. If each side has sent a request to the
other, and each side is unwilling to respond to the other side's
request until its own has been answered, then the protocol will
deadlock. Worse than that, if each side sends a fresh request in the
process of trying to answer the previous one, an endless sequence of
requests will fly back and forth, and both sides will run out of
memory in a manner very similar to an infinite recursion between the
two sides of a function-call interface.

(If the protocol does not number requests and responses, instead
relying on responses being sent in the same order as the requests
they answer, then disaster will strike sooner than that. Side A
sends request A1; side B sends back request B1; side A needs to send
request A2 in order to work out the answer to B1. But if responses
must be sent in the same order as requests, then this is already a
disaster: side A cannot possibly receive response A2 before it has
received response A1, yet this is exactly what needs to happen to
unblock the protocol.)

\H{bidi-racecond} Preventing race conditions

Another difficulty arises if a network protocol is written in terms
of objects which both sides can manipulate. I'll illustrate this
with an example from SSH: the channel architecture.

SSH is able to multiplex a potentially large number of \q{channels}
- two-way communications pathways - over a single encrypted TCP
connection. So any data flowing over the SSH connection must be
tagged with an identifying number so that the recipient knows which
channel it was part of, and knows where to send it on to. Either
side of the connection (server or client) can request the creation
of a channel; the protocol is symmetric at this level, so the \q{new
channel} request sent looks the same in both directions, and the
\q{success} and \q{failure} responses sent back are identical as
well.

The question is: how are channel numbers allocated? The obvious
answer is that when either side requests a new channel, it invents a
channel number and sends that as part of its request. But this leads
to a fundamental problem - what happens if both sides request a
channel at the same moment, and both decide to allocate the same
channel number? The requests will pass each other in the network,
each side will receive a request that uses what it thinks is an
already-allocated channel number, and both sides will therefore
reject the other's request. A better scheme is needed.

SSH solves this by means of having two sets of channel numbers: one
set is managed by the client, and the other one by the server. Every
channel has a number assigned at each end. So when the client wants
to open a channel, it allocates a number from its own number space,
and sends a \q{new channel} request quoting that number. If the
server is willing to open the channel, it allocates a number from
\e{its} number space, and sends back a response saying something
like \q{Your channel 123 is open; my number for it is 456}. Then
whenever the client sends data to the server for that channel, it
quotes the server's reference number (456); and when the server
sends data to the client for the same channel, it quotes the
client's reference (123). So each side of the connection has sole
authority over its own number space, and can choose any way it likes
to allocate numbers within that space.

This isn't the only way to solve the problem. Another approach might
have been to partition the channel numbers (into odd and even,
perhaps, or into a first half and a second half), and rule that each
side had to allocate channel numbers from a particular half. That
way the two sides could never simultaneously allocate the same
number, and the race condition would be prevented. However, SSH's
approach has two advantages:

\b In SSH, each side of the connection is always told \e{its own}
reference number for a channel, so it never has to look a channel up
using the other side's number. So if a particular implementation
wants to make life easy for itself by having a hard limit of (say)
ten channels, keeping them in a ten-element array, and choosing a
number for a new channel simply by scanning the array for a free
slot, then it can do that with no difficulty. Whenever it needs to
look up a channel by number, the number will always be its own
reference, so it need only do a quick array-index operation. By
contrast, if half the channel numbers it was dealing with were
allocated by the other side, it would be forced to have a general
means of storing a set of arbitrary integers: perhaps a B-tree, or a
hash table. This would introduce more complexity than simple
implementations would want to deal with.

\b Partitioning the available channel numbers in half limits the
number of channels which can be allocated by a particular side to
half of the total number of possible channels. SSH's method allows
one side to allocate \e{all} the available channels if it happens to
want to. (In SSH this is academic, since no known application would
ever need 2^32 channels! But if the channel numbers were shorter,
this might be a practical concern.)

\C{compatibility} Compatibility

Most interfaces are not perfect when they are first designed. Most
of them need to be redesigned later, or at least changed in some
way.

If your interface is a network protocol, or an interface to a shared
library which can be upgraded independently of the programs which
depend on it, then making changes to the interface becomes more
interesting. You can't necessarily guarantee that both sides are
going to be built to conform to the same version of the interface
specification. A robust interface can do a lot to make this easier.

The two big questions in compatibility are opposite sides of one
another:

\b Can each side behave sensibly if the other side is operating on
an older version of the interface?

\b Can each side behave sensibly if the other side is operating on a
newer version of the interface?

In the simplest case, compatibility implies that you should not
change the behaviour of an existing request or response. If a new
version of the interface specification states that the server is
allowed to respond in a new way to a particular request, for
example, then servers which take advantage of this will confuse
clients which don't know about it.

Extending the protocol rather than changing it is often a good way
to proceed. Adding a new type of request in a network protocol, or
example, is fairly safe. Clients which understand the new request
will have to be aware that older servers may return a \q{request not
recognised} error, and be prepared to fall back to older request
types if this happens, but other than that it's a pretty safe way to
proceed. Of course, it doesn't work so well in shared libraries,
where adding a new function will cause programs which use it to fail
to link to the old version of the library.

\H{compat-negotiation} Negotiation

If you do need to extend the protocol in more interesting ways, or
you really do need to change existing behaviour, there are ways to
arrange for this not to break compatibility. These ways involve
somehow \e{negotiating} a common set of protocol features between
the two sides.

One option is to number the versions of your protocol. Then one or
both sides announce which version they support at the start of the
conversation, and each side can change its behaviour as appropriate
to deal sensibly with the other. When you create a new version of
the protocol in which the behaviour of existing protocol elements
changes, you simply assign it a new number, and any new
implementation which sees a version number lower than that knows to
expect the old behaviour. (For backwards compatibility, you usually
want the newer side to downgrade its behaviour so that the older
side can understand.)

Another approach, if you're foresighted enough to build
extensibility into the protocol from the start, is to negotiate
specific protocol features individually. SSH, and its associated
file transfer protocol SFTP, use this approach in many places:

\b At the start of an SSH connection, each side sends the other a
list of the cryptographic algorithms it has available for key
exchange, bulk encryption, integrity protection and so on. The two
sides then agree on a set of algorithms supported by both. So either
side can add new algorithms at any time, and they will be used when
talking to another implementation which supports them and ignored
the rest of the time.

\b During the session, either side can send requests to the other
side, which are identified by names. If the other side doesn't
support that particular feature, it simply returns failure.

\b In SFTP, extensions to the protocol can be negotiated at the
beginning of a session. If the client and server both agree on such
an extension, this can cause subsequent parts of the protocol to
change in incompatible ways.

It's worth noting SSH's approach to naming, as well. All the above
SSH features (cryptographic algorithms, session requests, SFTP
extensions and others) are identified by strings. Anybody can invent
their own features and assign their own identifying strings. To
prevent clashes between people assigning the same name to two
different features, the names are of the form
\c{feature-name@domain}, where \c{domain} is an Internet domain such
as \c{ssh.com}. The owner of any given domain has authority over
what SSH protocol features are given names ending in that domain; so
if you want to be sure nobody is going to clash with your names, you
simply acquire a domain and name features within that domain.
Features standardised by the IETF are identified by names that do
not contain an \c{@} sign.

\H{compat-proxy} Proxy-friendliness

As well as considering whether a client can deal with an older or
newer server, and whether a server can deal with an older or newer
client, there is a third dimension to compatibility in network
protocols, which involves proxies.

Suppose I'm writing an NNTP proxy, which passes most requests
straight through from the client to the server, and passes most
responses straight back in the same way. This would be very easy if
I wanted to pass \e{all} requests and responses through unchanged;
but I wouldn't be writing the proxy if I didn't want to alter some
of them. Now If I want to intercept particular NNTP commands and
deal with them myself rather than passing them on to the server, I
need to know which of the lines sent by the client \e{are} commands
- if I respond to any line that says \c{GROUP foo.bar}, for example,
then things will go horribly wrong if the user attempts to post a
news article through my proxy which contains that line (which I'm
likely to do almost immediately when I post an article talking about
my new news proxy!).

So I have to be able to distinguish between a line sent by the
client which is a command, and a line sent which is part of an
article body. In the other direction, I might also need to
distinguish server lines which are responses or part of article
bodies. In order to do \e{this}, I need to know when I should expect
the client or the server to be sending an article.

NNTP is not well designed for this. RFC 977 \#{FIXME: reference}
defines particular response codes which the server may send to
indicate that an article follows, and other response codes which the
server may send to indicate that the client should send an article.
However, it does not \e{limit} the range of those codes; a future
extension to NNTP could easily add extra codes which caused unusual
data to be transferred, and that extension might break a proxy which
didn't know about it.

If NNTP had been designed to allow for this, RFC 977 might have
contained statements which said

\b Response codes in the range 215-234 inclusive are followed by
further lines of text, terminated by a period on a line by itself.

\b Response codes in the range 330-349 inclusive indicate that the
client should send further lines of text, terminated by a period on
a line by itself.

\b All other response codes indicate the end of processing for that
command.

If further extensions to NNTP were required to fit within that
framework, then I could be confident that as long as my proxy
followed those rules, it would never interpret a line of text as a
command or a response unless it was really intended to be one.

In general, then, the guideline for a protocol to be proxy-friendly
is that the initial specification of the protocol should lay down a
simple and fixed set of rules governing the overall shape of the
protocol, in as much detail as a proxy might reasonably need to know
it. Any old proxy dealing with newer versions of a protocol is going
to encounter commands and responses which it doesn't understand the
exact meaning of; but it should at least be able to know how to pass
them through unchanged without losing track of the rest of the
session.

\# FIXME: perhaps mention PNG as a shining example here? Would need
\# to excuse it not being a protocol or interface as such :-)

\C{threading} Multithreading and thread-safety

Interfaces usually become more complex once you introduce the
possibility that multiple threads of control might try to use the
same interface at the same time.

In function-call interfaces, the obvious question when you have
multiple threads is: under what circumstances are two threads
allowed to use the interface at the same time? Does it make a
difference if the two threads are calling different functions in the
same interface, or the same function? If the interface defines some
sort of object (such as the \q{contexts} in the example in
\k{what-ordering}), then does it make a difference if the two
threads are calling interface functions for the same object or
different ones?

In network protocols, this sort of difficulty is much less likely to
lead to program crashes, since whether or not one party is
multithreaded is usually independent of whether the other one is.
However, there are still things a network protocol can do to make
life easier for a multithreaded program trying to deal with it.

If the client side of a protocol is multithreaded, then more than
one thread is likely to want to send requests and receive responses.
It would be difficult to have all those threads receiving data
directly from the network connection, so what would probably happen
is that each thread would send its requests to a central thread
responsible for talking to the network. That would transmit the
requests down the actual connection, and when it received a response
it would send the response back to the thread that had sent the
corresponding request. Now if the server is capable of \e{delaying}
before sending a response, and if responses must be sent in the same
order as requests, then this might cause client threads to be
blocked for longer than necessary. In this situation it might well
be better to number requests and responses in some way, so that
responses can arrive in a different order from requests. That way,
each client thread only delays for as long as it has to.

Another thing that makes multithreaded use of an interface harder is
implicit state information. FTP, \#{FIXME: RFC 959} for example, has
a concept of the server's current directory: if you want to issue a
command that deals with a particular directory, you usually have to
send a CWD command first to set the server's current directory. This
means that if two client threads are trying to share the same FTP
connection while working in different directories, they are likely
to need to send a CWD command in between every pair of requests.
SFTP, by contrast, deliberately avoids having a \q{current
directory} concept; instead, enough information is made available to
the client that it can simulate one if it needs it. Every request
sent to an SFTP server is processed in exactly the same way no
matter where in the connection it is sent. So two threads sharing an
SFTP connection can easily work in two different directories,
without causing each other inconvenience.

(Of course, SFTP pays for its thread-friendliness by requiring more
data transfer to do the same amount of work.)

\C{misc} Miscellaneous issues

This section lists a few things to consider which didn't fit
sensibly into other sections.

\H{misc-speed} Speed

Speed is always a potential consideration in programming. It doesn't
affect interface design as much as it does other areas, but it's not
completely irrelevant either.

In a function-call interface between modules of the same program,
speed isn't a major problem at all. In most situations, the amount
of computation done on each side of the interface is going to be
much larger than the overheads of the function calls made across the
interface. The only exception is if the interface provides a
function which handles very small pieces of data, so that it has to
be called a very large number of times. For example, \cw{fputc()} in
the C library might introduce a noticeable speed penalty if it were
the only way to write a character to a file - because a 1Gb file
would require a billion function calls to write. Hence the C library
supplies \cw{fwrite()} as well, to cut down the number of function
calls to a fraction of the number of bytes written.

In networked interfaces (either conventional network protocols or
RPC-style systems designed to behave like function calls), speed is
likely to be more of a problem, since the network is probably more
limited than the CPU at each end of the connection. In this
situation there are two separate speed issues to consider:

\b Bandwidth: the total amount of data transferred over the network
connection.

\b Round trips: the number of times side stops to wait for the other
side to reply to it. Increasingly, modern networks tend to have high
bandwidth but also high round-trip times; so if you transfer a large
amount of data in small blocks, but you stop and wait for a reply
from the other end between each pair of blocks, then you will use
only a fraction of the available bandwidth in the connection.

\H{misc-privacy} Keep each side's state on that side

In an interface where each side has limited trust in the other side,
it's almost never a good idea to ask one side to store the other
side's private state for it.

This may sound obvious, but at least one reasonably modern protocol
didn't think so. Version 1 of the SSH protocol implements remote
port forwarding in a way that violates this principle.

In remote port forwarding, the SSH server listens for connections on
a particular network port. When a connection comes in, it tells the
SSH client, which then makes a connection to a local server of some
kind, and data is exchanged between the process which connected to
the SSH server and the process the SSH client has connected to.

In SSH v1, the SSH client sets this up by sending a message to the
server saying \q{Please listen on port 2345. When a connection comes
in, I will send it to \cw{myserver.example.com} port 3456.} The SSH
server listens to the requested port, and when it receives a
connection it sends back a message saying \q{Please connect to
\cw{myserver.example.com} port 3456}.

This approach has two problems:

\b It's an invitation for client implementations to do the wrong
thing. The obvious naive client implementation will simply listen
for any \q{please connect} messages and connect to the address they
specify. If a client like this connects from inside a firewalled
zone to a server outside it, it has just given the system
administrator of the server machine the ability to connect to
\e{any} service provided behind the firewall. So clients \e{must}
maintain a list of addresses they have asked the server to send
\q{please connect} messages for, and must verify that any real
\q{please connect} message has an address in this list.

\b Even if the client doesn't make that mistake, it's an invasion of
privacy; the location of services in the client's network might be
no business of the server administrator, so the client might not
want to tell the server what they are. In this situation, the client
might be well advised to tell lies - to make up fictitious service
addresses to send to the server, and to translate them into the real
ones when they come back.

SSH version 2 does this the right way. The client isn't even
\e{asked} to send its local service address to the server. Instead,
it simply tells the server \q{Please listen on port 2345}, and when
the server receives a connection it sends back \q{Here's a
connection on port 2345 as you requested}. Then the list of which
ports are forwarded to which local services is kept at the client
end, and the client looks up the port number in that list to decide
where to connect to. Now the only way the server can cheat is by
sending a port number that the client never requested - in which
case the client will fail to find it in its list, and either
complain or ignore it.

\# \H{misc-secondguess} Don't second-guess
\# a real biggie: Thou Shalt Not Enumerate Possible User Types And
\# Cater For Each Individually. I'm sure there must be a fantastic
\# example of this somewhere, but right now I can't think of one.

\C{summary} Summary

This article has presented a wide range of issues that might be
worth considering when designing an interface between two pieces of
software, whether that interface is in the form of function calls, a
network protocol, or even other forms such as a file format.

I don't claim that everybody must always design interfaces which
satisfy all these criteria, but I do think it's generally better to
have considered an issue and decided you don't need to do anything
about it than not to have considered it at all.

I'll end this article by summarising all the points I've made into a
quick-reference checklist of things to think about when designing an
interface.

\b Is it worth designing a well-defined interface at all? Do the
advantages outweigh the cost?

\b If an interface separates interchangeable parts of a program from
the rest of the program, that's a strong condition for deciding on
which side of the interface to put a particular piece of code.

\b When there is limited trust between the sides of an interface,
position the interface \e{at} the trust boundary. Don't put code on
one side of the interface which the other side can't trust it to run
correctly.

\b If all other conditions are equal, it's probably a good guideline
to place your interface so as to minimise data flow across it (the
orange-segment model).

\b In every request and response, specify exactly what parameters
are allowed, forbidden and undefined.

\b When there is limited trust between the sides of an interface,
\e{no} parameter passed in a request or a response must cause a
security hole, even if the behaviour caused by that parameter is
otherwise undefined.

\b Specify the ordering constraints, if any, between requests and
responses. It might help to draw a state diagram to ensure you have
considered all the possibilities.

\b When pointers to areas of memory are passed across the interface,
specify which side owns them after that (both in the sense of having
the right to modify them, and - where applicable - in the sense of
having responsibility for freeing the memory).

\b If both sides of the interface can make requests of each other,
specify what requests may be made in response to what other
requests, and ensure this can never lead to endless recursion (in
function-call interfaces) or deadlock (in network protocols).

\b Be sure to prevent race conditions in a network protocol.

\b If the two sides of the interface can be upgraded independently,
ensure each side can cope with talking to an older or newer version
of the interface/protocol. You may wish to do this by stating the
protocol version number explicitly, or by negotiating individiual
protocol extensions.

\b In a network protocol, ensure proxy authors have enough
information to write proxies which can handle future protocol
extensions without losing track of the state of the session.

\b Consider multithreading. Can more than one thread use the same
instance of the interface at once? In a network protocol, is
implicit state kept between requests which would make this
difficult?

\b For speed reasons, don't arrange for very large amounts of data
to be handled by passing one byte at a time across the interface.

\b When considering speed in a network protocol, consider both the
amount of data transferred \e{and} the number of network round trips
required.

\b When there is limited trust between the sides of an interface,
don't recommend that either side of the interface gives its private
data to the other for storage.

\# -------------- end ---------------

\# review questions:
\#  - FFS give me a better title! I want to call it Interface Design
\#    Guidelines while making it clear it isn't about _user_
\#    interfaces. How hard can that be?
\#  - Have I aimed it at the right level of reader? Does the level
\#    I've aimed it at fluctuate wildly within the article?
\#  - Use of 1st, 2nd person: helpfully personalising, or confusing
\#    and inconsistent?
\#  - Excessively flowery and idiomatic English? Will foreign
\#    readers have trouble? (Probably.) Do I care? (Don't know.)
\#  - Are there far too many lengthy examples, or do they help make
\#    my points?

\# ------------------------------------------------------------

\# original notes follow
\#
\#  - When to use an interface
\#    + do pieces _need_ to be separate?
\#    + do some pieces need to be interchangeable?
\#    + will it aid debugging / development / analysis?
\#    + will it cost more to invent and test the interface than just to
\#      write the code as one lump?
\#
\#  - Where to put interfaces
\#    + interchangeability is a very strong condition for interface
\#      placement
\#      * for virtually any question about which side of the interface
\#        something should fall on, there is a strong argument for
\#        deciding the question according to whether it needs to change
\#        when the interchangeable side is switched
\#    + minimum-data-flow (orange segments) model
\#    + obviously physical separation (into different programs, on
\#      different computers, etc) is a strong indicator
\#    + trust in computation
\#      * for example, scp versus sftp in server-side wildcards. Orange
\#        segment analysis says the server should glob its own
\#        wildcards rather than transmitting the whole directory; but
\#        trust analysis says the client needs to be able to verify the
\#        server has done it right. Conclusion: of the two protocols
\#        sftp is superior _even though_ less efficient. Better still
\#        for this purpose would have been to standardise a wildcard
\#        syntax, so that the server can do the globbing
\#        (orange-segment compliant) and the client can verify the
\#        responses (secure). But note that this doubles the amount of
\#        computation! Trust requires a tradeoff between computation
\#        and network transfer. Such is life.
\#
\#  - Types of interface
\#    + client/server
\#    + bidirectional
\#      * as network - requests and responses flow in both directions
\#        + asynchronous - either side may make a request at any time
\#          - ensure the protocol isn't thrown by race conditions if
\#            both sides initiate a transaction at once and they cross
\#            en route.
\#        + synchronous - the server will only make a request if extra
\#          information is needed to fulfill a request from the client
\#      * as function calls - need _callbacks_
\#        - vital question: _when_ might these callbacks be called?
\#          Constantly, on signals or timers? In a separate thread? At
\#          defined points (eg a Windows window procedure is only
\#          called as a result of DispatchMessage, but almost no
\#          Windows programmers actually know that!)
\#        - in OO, between two communicating objects, this issue is
\#          rather more extensive. Both objects can call methods of the
\#          other, either of which can call back to methods in the
\#          first before returning. It's not clear _exactly_ how much
\#          the interface needs to document about what callbacks might
\#          and might not happen, but there certainly needs to be
\#          enough specified to ensure one method call can't kick off
\#          an unbounded chain of mutual recursion between the objects.
\#
\#  - Specifying requests and responses
\#    + obviously, specify the range of allowables in each request and
\#      each response
\#    + also - this is vital and frequently neglected - specify the
\#      _ordering_ constraints on requests and responses.
\#    + in non-GC languages, what about memory management?
\#      * if one side passes a piece of memory to the other, has it
\#        been passed for reference only, or has _ownership_ (in the
\#        sense of responsibility for freeing it) been passed?
\#      * if one side passes a piece of memory to the other but retains
\#        ownership, for how long can the other side expect to continue
\#        to refer to it? Can it keep the pointer and refer to it in
\#        subsequent calls, and rely on it not being freed until some
\#        well-defined future moment? Going the other way, can it rely
\#        on it not being freed, frobbed or fiddled with if it's forced
\#        to make a call back to the first side in the course of
\#        answering the original request?
\#
\#  - Protocol-level state
\#    + state not directly relevant to either party, introduced solely
\#      for the purpose of the protocol, as a common language
\#      * security advantage: this avoids revealing any details of
\#        either side's implementation to the other
\#        - though even in a less enlightened protocol, a decently
\#          paranoid implementation can of course preserve its secrets
\#          by lying
\#        - but it had better be sure the other guy isn't _relying_ on
\#          aspects of the fictions it invents
\#        - protocol-level identifiers are by definition content-free
\#          pure names, so neither side is in any danger of thinking
\#          they can get anything out of it.
\#      * neutrality: if one side invents descriptors for objects based
\#        on its own criteria, then the other side is prohibited from
\#        ever initiating any such object. Protocol-level descriptors
\#        are neutral and can be invented by either side.
\#    + in a network protocol: protocol-level state can be `owned' by
\#      one side or the other, removing race conditions when inventing
\#      a new instance. cf channel numbers in SSH2.
\#    + much simpler things than this, e.g. sequence numbers
\#    + there is of course implicit state in any protocol with ordering
\#      constraints; the client and server must both be keeping track
\#      of ordering issues. The information is never explicitly
\#      exchanged but both sides' representations of it must keep in
\#      step.
\#    + is state necessary? E.g. FTP has a current directory but SFTP
\#      doesn't. Result: SFTP is less network-efficient but much easier
\#      to interleave several threads of operation in a single
\#      protocol. See also strtok, in the multithreading section.
\#
\#  - Compatibility
\#    + is each party designed to cope with older parties (backward
\#      compatibility)?
\#    + is each party designed to make it easy for newer parties to
\#      cope with it (forward compatibility)?
\#    + can the protocol deal with client being older than server?
\#    + can the protocol deal with server being older than client?
\#    + version negotiation, with backwards compatibility to older
\#      versions of a protocol
\#    + optional features negotiated before use
\#      * perhaps with expandable namespaces
\#      * inherent extensibility; if this is done properly, it largely
\#        _removes_ the requirement for version negotiation.
\#    + proxyability.
\#      * consider NNTP: some responses say `please send an article',
\#        others say `command complete'. It isn't obvious which! The
\#        NNTP RFC defines a few response codes to which the user is
\#        expected to respond by sending an article, but doesn't rule
\#        out the introduction of others in future. This makes it hard
\#        to write a future-proof NNTP proxy. Better would have been a
\#        basic division of response codes into categories, and a
\#        decree that future response codes must be allocated from the
\#        correct category.
\#
\#  - Multithreading
\#    + in function call interfaces, which interface functions are
\#      re-entrant?
\#      * at all?
\#      * when called on the same interface-level object (if the
\#        interface defines independent objects within its definition)
\#    + in network-style interfaces, how easy is it to write a
\#      multithreaded client (or server) multiplexing down the same
\#      connection?
\#      * for example, numbering all requests and responses so the
\#        responses can arrive out of order makes it easier to write a
\#        multithreaded server
\#    + how much protocol state is implicit?
\#      * implicit state can inhibit multiple threads making
\#        independent uses of the interface. cf strtok. See also FTP
\#        versus SFTP, in the protocol state section.
\#
\#  - Speed considerations
\#    + in a network: quantity of data flowing across the interface is
\#      not the only issue. Number of round trips is also worth
\#      designing to reduce; modern networks are increasingly high-
\#      bandwidth-high-latency.
\#    + in a function-call interface: this is why we have fread() as
\#      well as fgetc(). Avoiding _too_ many function-call overheads
\#      is generally helpful.
