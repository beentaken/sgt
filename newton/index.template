<!-- -*- html -*- -->
<html>
<head>
<title>Fractals derived from Newton-Raphson iteration</title>
</head>
<link rel="stylesheet" type="text/css" href="../sitestyle.css" name="Simon Tatham's Home Page Style">
<body>
<h1 align=center>Fractals derived from Newton-Raphson iteration</h1>

<h2>Introduction</h2>

<p>
This page describes a type of fractal derived from the
Newton-Raphson method, which is more normally used as an approximate
method of solving equations.

<p>
This is not a new idea to me; I was given the idea by a colleague at
work, and several other people have web pages about it too. I'm
putting up yet another one because it contains some additions to the
concept which I haven't seen anywhere else.

<h2>Explanation</h2>

<p>
Newton-Raphson iteration should be familiar to anyone who has
studied calculus; it's a method for finding roots of a function
by using the derivative of the function to improve an approximation
to the root.

<p>
To perform a Newton-Raphson approximation, suppose you have a
function <code>f(x)</code>, with derivative <code>f'(x)</code>, and
you have an approximation <code>a</code> to a root of the function.
The Newton-Raphson procedure is to calculate
<code>a'&nbsp;=&nbsp;a&nbsp;-&nbsp;f(a)/f'(a)</code>, which is a
closer approximation to the root. Typically you would then iterate
this again, and again, until the successive values were
extremely close together, at which point you would conclude that you
had a very good approximation to the actual value <code>r</code> for
which <code>f(r)</code> equals zero.

<p>
The Newton-Raphson method is useful in practice because of its
extremely fast convergence. The distance from the root to each
approximation is roughly squared at each iteration; so assuming the
distance is already small enough that this makes it smaller rather
than larger, you expect to <em>double</em> the number of correct
decimal digits in each approximation. So if you can find a
reasonably good approximation to begin with, Newton-Raphson can very
quickly give you an excellent one.

<p>
What the Newton-Raphson formula is essentially doing is drawing a
tangent to the curve at the point of the original approximation,
then following that tangent to where it crosses the x-axis. Since
any differentiable function looks close to a straight line when
viewed at sufficient magnification, this explains why it works so
well: the curve itself does not diverge from the tangent line by
very much, and the points where they cross the x-axis are very close
to each other. Hence, this technique massively improves an already
good approximation.

<p>
However, if you start with a really <em>bad</em> approximation, much
more interesting things happen. Suppose the function curves up from
one intersection with the x-axis and back down to another, like a
parabola; and suppose your initial approximation is somewhere near
the top of this arc. Now drawing a tangent to the curve and
following it to the x-axis will land you a huge distance away from
the roots of the function - and as your initial approximation
<code>a</code> crosses the maximum point of the curve, the
Newton-Raphson second approximation <code>a'</code> will flip from
one side of the roots to the other. In fact, as <code>a</code> moves
the relatively short distance across the maximum of the curve,
<code>a'</code> will cover <em>most of the real line</em>.

<p>
This sort of behaviour, expanding a small area into a large one, is
exactly the sort of behaviour we expect to give rise to self-similar
fractals. So if we were to start a Newton-Raphson iteration at each
point on the real line, run each iteration until it converged to
within a given tolerance level of a root, and then colour the
starting point according to <em>which</em> root it ended up at, we
might well expect to see fractal shapes.

<p>
Fractals on one line are not very interesting, however; so let's
work in the complex plane. The Newton-Raphson iteration still works
perfectly well there, so that's where I'll be generating my
fractals.

<h2>Illustration</h2>

<p>
Here's an example fractal, generated from the
polynomial <code>z^4-1</code> - so the four roots of the function
are at -1, +1, <code>-i</code> and <code>+i</code>.

<p align=center>
<picture simple>

<p>
In this image, we see a large boring area surrounding each root of
the function - as we would expect, since any point near a root will
converge rapidly to that root and do nothing interesting. But
<em>between</em> the areas of boring well-behaved convergence, we
see some beautiful fractal shapes. Let's zoom in on one of those
boundary areas:

<p align=center>
<picture zoomed>

<p>
Just as we predicted - each of the heart-shaped blobs making up the
boundary line is itself composed of boundary lines made up of
further heart-shaped blobs. This pattern is a true fractal.

<p>
Here's a rather different example. This time the function being
used is <code>(z-3)(z-2)(z-1)z(z+1)(z+2)(z+3)</code>, so it has
seven roots strung out in a long line:

<p align=center>
<picture rainbow>

<p>
In this case, the fractal shapes are much smaller compared to the
overall structure of the image. But they're not completely absent.
If we zoom in on a couple of the little blobs on the boundary lines,
we see this:

<p>
<table width="100%" border="0">
<tr>
<td align="center" valign="top" width="50%">
<picture rainbowz1>
</td>
<td align="center" valign="top" width="50%">
<picture rainbowz2>
</td></tr></table>

<p>
Each blob is divided up into coloured areas similar to those
covering the whole plane, and on each dividing line we see more
blobs looking much the same as the larger blobs.

<h2>Decoration</h2>

<p>
These images are reasonably pretty, but they're a bit garish to be
turned into desktop wallpaper in their current form. Is there
anything we can do to make them less stark?

<p>
Yes, there is. One obvious thing we can do, as well as noticing
which root of the function the iteration ended up at, is to count
how many iterations it took to get there. We can then colour each
pixel a different <em>shade</em> of the colour assigned to that root
depending on the number of iterations. So, using the obvious
approach of setting the pixel shade to the number of iterations
modulo the number of available shades (so that each colour cycles
through those shades), we see something like this:

<p align=center>
<picture shaded>

<p>
This is not only prettier, but it also shows us exactly where
each root of the function <em>is</em> - instead of just knowing the
roots are somewhere in the large coloured areas, we can now
positively identify each root as the centre of the bright spot in
each area.

<p>
The cyclic behaviour is not quite optimal, though; it works well
enough if the number of available colours is limited, but it means
there are sudden edges (like the ones at the very centre of each
region) where a dark colour suddenly becomes a bright colour again.
Perhaps if we have true colour available, it would be better to have
the pixel shading be monotonic - always getting darker the more
iterations are needed, but fading out by less and less and never
actually reaching blackness:

<p align=center>
<picture shaded2>

<p>
Now that's starting to look <em>much</em> nicer, I think. But it
would be even better if the visible boundaries between different
shades of the same colour could be removed. I suspect that doing
this rigorously requires some really horrible maths and a lot of
special cases, but I've found that a good ad-hoc approximation is
obtained simply by looking at the last iteration, in which the point
first comes within the specified distance of a root. We look at
the distance <code>D0</code> from the previous point to the root and
the distance <code>D1</code> from the new point to the root, and we
know that the threshold distance <code>T</code> is somewhere in
between the two. I've found that simply looking at
<code>(log T-log D0)/(log D1-log D0)</code>, in other words whether
the log of the threshold radius was near to the start or the end of
the inward distance travelled by the point (on a logarithmic scale),
produces a perfectly acceptable result which we can use to smooth
out those boundaries:

<p align=center>
<picture shaded3>

<h2>Animation</h2>

<p>
In order to write a program to generate these images, it's necessary
to know both the function being used (typically a polynomial) and
the exact locations of all its roots. Finding the exact roots of a
general polynomial is not easy (cubics and quartics are just about
solvable, but quintics and beyond fall foul of Galois theory), so it
makes much more sense to <em>start</em> by deciding where we want
the roots to be, and using that to compute the polynomial by
multiplying together a series of <code>(x-a)</code> terms. This does
not restrict the range of polynomials we can end up with, since in
the complex plane any polynomial can be fully factorised.

<p>
So the actual parameters you would pass to the fractal program
consist of the coordinates of a set of points, together with a
colour for each point. This led to an interesting idea: suppose we
imagine a small number of coloured points drifting gently around the
plane, and at each instant of time we compute a Newton-Raphson
fractal for the current positions of the points. This should lead to
a sequence of fractals which flow naturally on from each other, and
as the bright central point in each coloured region moves, the
regions move with them and the fractal phenomena on the region
boundaries swirl continuously.

<p>
Here is such an animation. To create this, I've set up three points,
each moving along the Lissajous curve
<code>(sin&nbsp;t,&nbsp;sin&nbsp;2t)</code>,
and each one third of the way further around the curve than the
last.

<p>
<a href="cascade.mpeg">[download cascade.mpeg, 320x256 MPEG
animation, 633KB]</a>

<p>
The image quality isn't amazing (due to the MPEG compression), but
one feature that's just about visible is the lines of additional
bright spots moving within each coloured region, which appear to
<em>become</em> the blobs on the connecting line when the other two
regions come together to squash the line of spots.

<h2>Evolution</h2>

<p>
Looking at the above rainbow-coloured plot with seven roots in a
line, I found myself thinking that it's not very pretty, because the
fractal blobs are so small. Why are they so small? Could anything be
done to make them bigger?

<p>
I speculated that perhaps the reason the blobs are so small
(implying that Newton-Raphson converges particularly well for this
function) might be because polynomials of a high degree are
generally very steep, and curve over very sharply at the top of
peaks, so there isn't a large region of the plane in which the
fractal self-similarity can be observed. This suggested than an
obvious way to soften the process might be to reduce the overall
degree of the function: to make it more like <code>x^4</code> than
<code>x^7</code>. How to do that while preserving the
locations of the roots? Why, take the square root, of course!

<p>
Thinking about this a bit further, that does sound like a good idea.
In the real numbers, the square root transformation squashes things
towards the x-axis, and it squashes them <em>more</em> the further
away they are; so I would indeed expect it to turn a very sharp
switchback into a more gentle curve.

<p>
Moreover, it's trivially easy to do in the Newton-Raphson iteration.
Suppose we have a function <code>f</code>, and we define a function
<code>g</code> to be <code>f^k</code> for some power <code>k</code>.
Then <code>g' = k f^(k-1) f'</code>; so when we compute
<code>g/g'</code>, the <code>f^(k-1)</code> term cancels out, and we
are left with simply <code>(1/k) . (f/f')</code>. In other words,
the sole effect of raising the entire function to the power
<code>k</code> is to multiply a constant factor of <code>1/k</code>
into the distance moved per iteration.

<p>
So this had to be worth a try. Here are the results from taking the
rainbow plot above, and raising the entire function to a variety of
overall powers:

<p>
<table width="100%" border="0">
<tr>
<td align="center" valign="top" width="25%">
<picture rainbowp1>
</td>
<td align="center" valign="top" width="25%">
<picture rainbowp0.75>
</td>
</tr>
<tr>
<td align="center">1.0</td>
<td align="center">0.75</td>
</tr>
<tr>
<td align="center" valign="top" width="25%">
<picture rainbowp0.6>
</td>
<td align="center" valign="top" width="25%">
<picture rainbowp0.52>
</td>
</tr>
<tr>
<td align="center">0.6</td>
<td align="center">0.52</td>
</tr>
</table>

<p>
As I had hoped, the small fractal blobs have expanded into larger
fractal blobs, which is good; and they've become more complex and
spiky in shape as well, because the smaller fractal blobs on
</em>them</em> have also expanded. What I wasn't expecting, however,
was that the dividing lines between the main convergence regions
have entirely changed shape, becoming more wiggly.

<p>
It turns out that actually taking the square root of the function
(raising it to the power 0.5) is <em>not</em> feasible, since the
algorithm converges more and more slowly the closer to 0.5 you get.
This is because raising the function to the power 0.5 causes us to
double the distance moved per iteration; so once we're near to a
root, <code>f/f'</code> gives you an accurate estimate of the
distance to the root, and we go <em>twice as far</em>, causing us to
end up just as far away from the root on the far side, so no wonder
the iteration fails to converge. 0.52 was the smallest value I could
use without having to increase my iteration limit.

<p>
This tendency to overshoot the root we're aiming at when using
<code>k&lt;1</code> also offers an alternative intuitive explanation
for the fractal shapes becoming bigger. The fractal shapes arise
because near to a boundary between convergence regions there is a
tendency for a single iteration step to travel a long distance into
remote regions of the complex plane; so if we're moving even further
in a single iteration, then we would indeed expect not to have to be
so close to the boundary before experiencing this phenomenon.

<p>
Raising the value of <code>k</code> to something <em>greater</em>
than 1 has the opposite effect: we now converge toward our root in
smaller steps, more slowly but more surely, and so we're less likely
to overshoot and the fractal effects on the boundary lines become
less pronounced. I'm not going to exhibit any pictures of that,
because they're boring. However, I'll come back to this later.

<p>
As well as raising the <em>entire</em> polynomial to a power, it's
also interesting to see what happens if we try raising only one of
the <code>(z-a)</code> factors to a power. To do this we must first
think about whether this can be efficiently implemented in software.

<p>
It can, it turns out. Observe that if you have a function made up of
the product of a lot of smaller factors
<code>f = abcd</code>, then the product rule gives its derivative as
the sum of a list of terms <code>f' = a'bcd + ab'cd + abc'd +
abcd'</code>, in which each term looks very similar to <code>f</code>
itself. In fact we can divide both sides by <code>f</code> to give
us <code>f'/f = a'/a + b'/b + c'/c + d'/d</code>, which is precisely
the thing whose reciprocal we subtract from <code>z</code> in the
N-R formula. Now when each factor <code>a</code> is of the form
<code>(z-k)</code>, then the resulting term <code>a'/a</code> looks
like <code>1/(z-k)</code>; and as we observed in the previous
section, if the factor is raised to a power so that it's of the form
<code>(z-k)^e</code> then the resulting term <code>a'/a</code> is
only altered by a constant, so it becomes <code>e/(z-k)</code>.

<p>
So if we have a function which is the product of a number of terms
of the form <code>(z-a1)^b1</code>, then we can efficiently perform
its Newton-Raphson iteration without ever computing the entire
function itself. We simply compute <code>b1/(z-a1)</code> for each
term, sum them, take the reciprocal, and subtract that from
<code>z</code>.

<p>
A useful point about this procedure is that it means we can
conveniently raise each factor of our "polynomial" (which isn't very
polynomial any more, really) to not only an arbitrary <em>real</em>
power, but to a <em>complex</em> power if we so wish. And it turns
out that we do so wish, because some very impressive fractals turn
up if we do. Here's a map of 121 small fractal images, all generated
from functions with the same three roots, namely the complex roots
of 1. But the red root (1 itself) has been raised to a different
power in each picture: across the map the real part of the power
runs from 0 to 1, and downwards the imaginary part runs from 0 to 1.

<p align=center>
<picture powermap>

<p>
As mentioned above, raising a root to a power of 0.5 or less
inhibits convergence of the iteration to that root at all. But in
the presence of other roots to which we <em>can</em> still converge,
the region of non-convergence - shown in black on the above map -
forms complex and interesting fractal shapes. Meanwhile, applying an
imaginary part to the power of the red root causes a twisting
effect: the higher the power, the more the shape is less straight
and more spirally; and it appears that it's the <em>real</em> part
of the power which must be greater than 0.5 to manage to converge.

<p>
Some of the images in that map are well worth expanding to a larger
size to admire in more detail. Here are four particularly good ones:

<p>
<table width="100%" border="0">
<tr>
<td align="center" valign="top" width="25%">
<picture power1>
</td>
<td align="center" valign="top" width="25%">
<picture power2>
</td>
</tr>
<tr>
<td align="center"><code>0.5</code></td>
<td align="center"><code>0.5+0.3i</code></td>
</tr>
<tr>
<td align="center" valign="top" width="25%">
<picture power3>
</td>
<td align="center" valign="top" width="25%">
<picture power4>
</td>
</tr>
<tr>
<td align="center"><code>i</code></td>
<td align="center"><code>0.4+0.9i</code></td>
</tr>
</table>


<h2>Imitation</h2>

<p>
If you look again at the images in the above section, you may notice
that some of the shapes - particularly the ones that occur when
things are raised to real powers near 0.5 - are beginning to look
interestingly like the shape of the Mandelbrot set. This suggested
to me that there might be some sort of close relationship between
these fractals and Mandelbrot/Julia sets.

<p>
So an obvious question to ask along these lines is, is there a
function to which we could apply the Newton-Raphson formula and end
up with the Julia set iteration <code>z := z^2+c</code>?

<p>
Well, if you write down the equation <code>z - f(z)/f'(z) =
z^2+c</code>, or equivalently <code>z - f/(df/dz) = z^2+c</code>,
then it looks a lot like a differential equation. If only we were
working in the positive reals instead of the complex numbers, we
could solve it quite easily by separating variables into <code>df/f
= -dz/(z^2-z+c)</code>, factorising the denominator of the RHS and
turning it into partial fractions, and integrating to get <code>f =
(z-a)^(1/(b-a)) . (z-b)^(1/(a-b))</code>, where <code>a</code> and
<code>b</code> are the roots of the quadratic <code>z^2-z+c</code>.
(Unless <code>c=1/4</code>, which is a special case arising from
<code>z^2-z+c</code> being a perfect square and looks rather
different; but I'll ignore that, because it's not a very interesting
Julia set anyway.)

<p>
And it turns out that even in the complex numbers this answer works
plausibly. The function <code>(z-a)^(1/(b-a)) .
(z-b)^(1/(a-b))</code> is of precisely the form discussed in the
previous section: it's the product of linear terms raised to
arbitrary powers. Hence we can conveniently do the Newton-Raphson
iteration for this function by the method described above: compute
<code>1/((b-a)(z-a)) + 1/((a-b)(z-b))</code> and subtract its
reciprocal from <code>z</code>. If you do that, remembering that by
construction <code>a+b=1</code> and <code>ab=c</code>, you will
indeed find that a lot of algebraic mess cancels out and you end up
with the iteration <code>z := z^2+c</code>.

<p>
So there is a family of functions to which the application of the
Newton-Raphson formula yields a Julia set iteration, and moreover
those functions aren't very far away from the ones we've already
been considering here. To prove it works, here are a couple of plots
of Julia sets, with their corresponding Newton-Raphson plot:

<p>
<table width="100%" border="0">
<tr>
<td align="center" valign="top" width="25%">
<picture julia1>
</td>
<td align="center" valign="top" width="25%">
<picture julia2>
</td>
</tr>
<tr>
<td align="center">Julia set for <code>0.28+0.528i</code></td>
<td align="center">Julia set for <code>-0.656+0.272i</code></td>
</tr>
<tr>
<td align="center" valign="top" width="25%">
<picture njulia1>
</td>
<td align="center" valign="top" width="25%">
<picture njulia2>
</td>
</tr>
<tr>
<td align="center">Newton-Raphson fractal for<br>
<code>(z-(0.9994-0.5286i))^(-0.4722-0.4998i) *<br>
(z-(0.0006+0.5286i))^(0.4722+0.4998i)</code>
</td>
<td align="center">Newton-Raphson fractal for<br>
<code>(z-(1.4623-0.1413i))^(-0.5086-0.0747i) *<br>
(z-(-0.4623+0.1413i))^(0.5086+0.0747i)</code>
</td>
</tr>
</table>

<p>
You will have noticed, of course, that the colouring is very
different. The two types of fractal are using matching iteration
formulae, but a Julia set plotter concentrates on how long it takes
the iteration value to land outside a critical circle, whereas the
Newton-Raphson plotter is actually waiting for the iterates to
converge to a point, and is only incidentally observing what happens
when they don't. So you wouldn't actually want to throw away your
dedicated Julia-set plotting programs; but it's interesting,
nonetheless, that something very like Julia sets are a special case
of Newton-Raphson fractals.

<h2>Dissection</h2>

<p>
Another noticeable thing about some of the above fractals is that
some have much more fractal content than others. The original
fractal at the top of this page with roots at -1, +1,
<code>-i</code>, <code>+i</code> has four big regions of flat colour
meeting at the origin, and the self-similar nature of the fractal
causes that quadruple contact point to be replicated in other parts
of the plane. This gives rise to qualitatively more interesting
fractal phenomena than the plot with seven roots in a line, in which
all the large convergence regions are separated by boring curves
which never meet, and the fractal blobs reflect this structure.

<p>
So I wondered, is there a way that we can predict how the
convergence regions are going to be shaped, and thereby construct
polynomials which have triple or quadruple meeting points exactly
where we want them?

<p>
Yes, as it turns out, there is.

<p>
I observed above that raising a polynomial to a real power greater
than 1 has the effect of causing the iteration to take smaller steps
towards the root, which in turn causes convergence to be slower but
surer and reduces the incidence of overshoot leading to fractal
phenomena. As a result of this, the boundary lines between regions
become clearer and smoother and simpler. This seemed like a good
thing if we wanted to know the overall structure of the fractal; but
merely <em>clearer</em> isn't good enough. I wanted <em>clearest</em>.

<p>
So I wondered what would happen "in the limit", as the iteration
speed slowed down more and more. In other words, I was interested in
what I'm going to describe as <em>continuous</em> Newton-Raphson,
in which you start your "iteration" at an arbitrary point
<code>z0</code> on the plane and then let <code>z</code> evolve
continuously according to the <em>differential equation</em>
<code>dz/dt = -f(z)/f'(z)</code>.

<p>
As in the previous section, we can "solve" this equation by
separating variables and na&iuml;vely integrating, trying hard to
ignore the question of whether this is rigorous or even meaningful
in the complex numbers. This time we end up with <code>1/f df/dz dz
= -dt</code>, and then we can apply the integration-by-substitution
formula to the LHS to give us <code>df/f = -dt</code>, which
integrates to give us <code>f(z) = Ae^-t</code> for some constant
<code>A</code>. We can check by differentiating with respect to
<code>t</code> that this does indeed turn out to be a plausible
answer to the equation even in the complex numbers: the chain rule
tells us <code>d/dt f(z) = f'(z) dz/dt</code>, so we end up with
<code>f'(z) dz/dt = -Ae^-t = -f(z)</code> as required. And the
constant <code>A</code> is clearly equal to <code>f(z0)</code>, the
value of <code>f</code> at the point where we started our integration.

<p>
"Very nice", I hear you protest, "but what does that <em>mean</em>?"

<p>
Well, since <code>t</code> is a positive real, it means that whatever
path <code>z</code> follows from our starting point <code>z0</code>
must have the property that at all times <code>f(z)</code> is equal
to <code>k f(z0)</code>, where <code>k = Ae^-t</code> is a real
value which continuously and monotonically decreases from 1 toward
0. So we normally expect the limit of such an "iteration" to be a
point at which <code>f(z)</code> is actually equal to zero, i.e. a
root of <code>f</code>.

<p>
But that's not the only thing that can happen. It's also possible to
imagine that we might encounter a point along this path at which
there isn't a clear direction we should head in: either there is no
direction in which we can head from <code>z</code> which will cause
<code>f(z)</code> to continue decreasing as a real multiple of its
initial value, or perhaps there is more than one such direction.

<p>
How do we find such points? Well, for most points in the complex
plane the differential equation we started with will usually give us
a unique direction in which we <em>can</em> head to linearly decrease
<code>f(z)</code>: we compute <code>f(z)/f'(z)</code>, and head in
the direction pointed to by that complex number. This fails if
<code>f(z)=0</code>, in which case there is no clear direction to
head in; we expect that, of course, because if <code>f(z)=0</code>
then we've already reached a root. But it can also fail if
<code>f'(z)=0</code>. So this suggests that the roots of the
<em>derivative</em> of <code>f</code> might be worth investigating.

<p>
So consider some root <code>r</code> of <code>f'</code>, at which
<code>f</code> itself is non-zero. If we can find any continuous
paths in the complex plane which start at <code>r</code> and have
<code>f</code> <em>increasing</em> as a real multiple of
<code>f(r)</code>, then any point on one of those paths will be a
point at which starting a continuous Newton-Raphson process will
cause it to head backwards along the same path and terminate at
<code>r</code> rather than at a root. In other words, we expect
those paths to be precisely the <em>boundaries</em> between the
convergence regions for the various roots (since that's the obvious
set of points which we expect not to converge sensibly to a root);
and moreover, on <em>every</em> such boundary we expect to find a
root of <code>f'</code> (because a continuous Newton-Raphson process
started on any boundary has to end up <em>somewhere</em>).

<p>
That's a lot of maths to endure without a break for a pretty
picture, and it's also a long and rather handwavey chain of
reasoning to endure without some sort of reassurance that what I'm
saying still makes sense. So, here I present a sample polynomial
Newton-Raphson fractal, with the roots of the polynomial's
derivative marked as black blobs. Observe that each dividing line
between convergence regions has a blob somewhere on it, and that in
particular the point where three regions (and three such lines) meet
has a blob.

<p align=center>
<picture droots>

<p>
So now we know, at least in theory, how to find the lines dividing
the different convergence regions. Now, what distinguishes a simple
dividing <em>line</em>, separating only two regions, from a point at
which three or more regions meet?

<p>
Let's consider our root <code>r</code> of <code>f'</code> again.
We're now interested in how many <em>directions</em> we can head
away from <code>r</code> in, such that <code>f</code> increases as a
real multiple of <code>f(r)</code>. Equivalently, we're interested
in directions we can head in such that <code>f(z)=(1+k)f(r)</code>
for some real positive <code>k</code>; in other words, we want small
values <code>e</code> such that <code>f(r+e)/f(r)-1</code> is real
and positive.

<p>
So now let's consider the Taylor series expansion for the function
<code>f(r+e)/f(r)-1</code>. <code>f(r+e)</code> is equal to
<code>f(r) + ef'(r) + e^2 f''(r) + e^3 f'''(r) + ...</code>, and we
know that <code>f'(r) = 0</code> by construction, so this gives us
<code>f(r+e)/f(r)-1 = e^2 f''(r)/f(r) + e^3 f'''(r)/f(r) +
...</code>.

<p>
In the usual case, <code>f''(r)</code> will be non-zero, so for
small values of <code>e</code>, <code>f(r+e)/f(r)-1</code> will be
approximately equal to <code>Ke^2</code> for some (complex) constant
<code>K</code>. This means that we expect to have <em>two</em>
directions in which we can head in order to make
<code>f(r+e)/f(r)-1</code> real and positive: one direction
corresponding to each square root of <code>1/K</code>. This fits
with what we expect, because in the usual case a root of
<code>f'</code> gives rise to a region boundary extending away from
it in two opposite directions.

<p>
But if <code>f''(r)</code> is zero as well, then the
<code>e^3</code> term will now be the dominating one in the Taylor
expansion; so for small values of <code>e</code>,
<code>f(r+e)/f(r)-1</code> will be approximately equal to
<code>Ke^3</code> for some <code>K</code>. And now we expect to have
<em>three</em> region boundaries coming out of our root
<code>r</code>, one for each cube root of <code>1/K</code>. If the
third derivative of <code>f</code> is zero at <code>r</code> too,
then we can have four region boundaries, and so on.

<p>
And there it is: that's the result I've been working towards for
this entire section. A multiple-region meeting point occurs when a
root <code>r</code> of <code>f'</code> is also a root of
<code>f''</code>, and the more higher derivatives are zero at
<code>r</code> the more regions meet there. For polynomials in
particular, this translates into <code>r</code> being a <em>repeated
root</em> of <code>f'</code>, and the more repeated the merrier.

<p>
To demonstrate beyond reasonable doubt that this actually works, I
will now construct from first principles a Newton-Raphson fractal
containing two points at each of which <em>five</em> regions meet.
For a five-way meeting point we need <code>f'</code> to have a
four-times-repeated root; so let's set <code>f'</code> to be
<code>(z-1)^4(z+1)^4</code>, which has two repeated roots as
desired. We "integrate" this in the na&iuml;ve way (actually what
we're doing is finding an anti-derivative of it, integration in the
complex plane being a generally messy concept) to obtain the
nasty-looking polynomial <code>(35z^9 - 180z^7 + 378z^5 - 420z^3 +
315z)/315</code>. We can discard the constant factor of
<code>1/315</code> since it makes no difference to the convergence,
and this gives us <code>35z^9 - 180z^7 + 378z^5 - 420z^3 +
315z</code>. We find the roots of this (using Newton-Raphson for its
more conventional purpose) and feed them into our fractal plotter,
and get this picture:

<p align=center>
<picture five>

<p>
And, exactly as we asked for, this has two five-way meeting points
at locations +1 and -1. Bingo!

<p>
Of course, integration lets us add an arbitrary constant term
without affecting the derivative of our result. Here's what we get
if we add a constant term of <code>50+200i</code> to the above
polynomial:

<p align=center>
<picture five2>

<p>
The roots of the polynomial have moved around, and the picture is
distorted, but the region meeting points are still exactly where we
asked for them.

<h2>Oscillation</h2>

<p>
Here's another curiosity. The following fractal is plotted using a
pure polynomial <code>f</code>, placing the roots at
<code>+i</code>, <code>-i</code>, <code>-2.3</code> and
<code>+2.3</code>:

<p align=center>
<picture holes>

<p>
The interesting feature of this picture is the black areas, which
are regions in which Newton-Raphson failed to converge to a root
within the program's limit of 256 iterations. At first sight one
might assume that this is an artifact of having a finite iteration
limit at all, but one would (it turns out) be wrong. Even if you
crank up the iteration limit to a much larger number, those black
areas stay black, because they represent genuine non-convergence.

<p>
What's actually happening here is that for this particular
polynomial the Newton-Raphson method gives rise to period-2 cyclic
behaviour. There's a pair of points <code>a</code> and
<code>b</code>, one in the middle of each of the two main black
areas, which have the property that a single iteration of
Newton-Raphson starting from <code>a</code> takes you to
<code>b</code>, and vice versa. But this is more than just a cycle
between those two points: it's an <em>attracting</em> cycle. If you
start from a point <em>somewhere near</em> <code>a</code>, an
iteration of Newton-Raphson will take you to somewhere <em>even
nearer</em> to <code>b</code>, from which another iteration will
land you nearer still to <code>a</code> again. Hence, there's a
sizeable region around each point of the cycle which all converges
to the cycle itself, and hence never settles down to a root of the
polynomial.

<p>
This is interesting to me because it happens so rarely; I've plotted
quite a lot of these fractals, including a lot of animated ones in
which the points wander continuously around the plane, and this
particular arrangement of roots is the <em>only</em> case I've
found in which a polynomial gives rise to a non-converging area.
(It's not the only actual <em>polynomial</em>: you can obviously
rotate or translate the root positions to obtain other polynomials
with the same behaviour, and you can also move the roots around by a
small amount relative to each other before the black areas go away.
But this general pattern - four roots in a diamond layout of
<em>roughly</em> this shape - is the only pattern I know of that
does this.)

<p>
On one level, it's reasonably easy to see "why" it happens.
If you define <code>g(x) = x - f(x)/f'(x)</code> to be the
Newton-Raphson iteration for this polynomial, then find roots of the
equation <code>g(g(x))=x</code> (probably using a computer algebra
package, since it's a pretty ugly mess) you will observe that it has
roots which are not also roots of the similar first-order equation
<code>g(x)=x</code> (and hence of <code>f</code>). That proves that
there are cycles at all; then, to prove there are
<em>attracting</em> cycles, compute the derivative of
<code>G(x)=g(g(x))</code> at those roots and find the ones where it has
modulus less than 1 (since for small <code>h</code>,
<code>G(r+h)</code> will be approximately <code>G(r)+G'(r)h</code>,
and if <code>G(r) = r</code> and <code>|G'(r)| &lt; 1</code> then
this will be closer to <code>r</code> than we started out).

<p>
But that doesn't really <em>explain</em> the phenomenon, or predict
what other sorts of polynomial will give rise to cyclic behaviour of
this type. Do there exist polynomials which exhibit
period-<code>n</code> cycles for all <code>n &gt; 1</code>, for
example? How often does this happen? How would one go about
constructing polynomials with non-converging regions to order?

<p>
I don't currently know the answers to these questions, but I'd be
interested to hear them if anyone else does.

<h2>Participation</h2>

<p>
If you want to generate some of these fractal images yourself, a
program to generate them is available for download
<a href="newton.c">here</a>. The program is
provided as C source code - you'll need a C compiler in order to use
it. For Windows users, Borland provide a C compiler for
free-of-charge download, on their web site at
<a href="http://www.borland.com/bcppbuilder/freecompiler/">www.borland.com</a>.

<p>
The program produces its output as Windows 24-bit .BMP files. Most
image processing software should be able to convert these to other
formats for you.

<p>
Just typing "<code>newton</code>" should give some help about what
all the command-line options do. If you want a quick start, here's a
selection of sample command lines you might like to try:

<ul>
<li>
<code>newton -o simple.bmp -s 256x256 -x 2 -c
1,0,0:1,1,0:0,0.7,0:0,0.5,1 -- -1 +1 -i +i</code>
<br>
This one is the cross-shaped plot I used at the top of this
page, based on the polynomial <code>z^4-1</code>. To zoom in on one
of the boundary lines as I did above, replace "<code>-x 2</code>"
with "<code>-x 0.4 -X 1.05 -Y 1.05</code>".

<li>
<code>newton -o rainbow.bmp -s 320x256 -x 5 -c
1,0,0:1,0.7,0:1,1,0:0,1,0:0,0.7,1:0,0,1:0.5,0,1 -- -3 -2 -1 0 1 2
3</code>
<br>
This one is the rainbow-coloured plot with seven roots strung out in
a long line.

<li>
<code>newton -o powermap.bmp -s 256x256 -x 2 -c 1,0,0:1,1,0:0,1,0 --
1/</code><em>power</em>
-0.5-0.8660254i -0.5+0.8660254i</code>
<br>
This command plots the various images from the large map shown
above, with the red root raised to an arbitrary power. Replace
<em>power</em> with the power you want; for example, replace it with
<code>0.5</code> to get the Mandelbrot-like picture, and with
<code>0.4+0.9i</code> to get the disconnected yellow-and-green plot.

<li>
<code>newton -o holes.bmp -s 320x256 -y 2 -c
1,0,0:1,1,0:0,0.7,0:0,0.5,1 -- -2.3 +2.3 -i +i</code>
<br>
This one is the plot with black non-converging areas.

<li>
By default the program will use the cyclic shading behaviour with 16
shades of each colour. You can specify <code>-C no</code> to turn
off cyclic shading (so the colours get uniformly darker as more
iterations are needed), <code>-B yes</code> to turn on blurring of
the iteration boundaries, and <code>-f 32</code> or <code>-f
64</code> if you want to increase the number of shades used.

<li>
To raise the function to an overall power, use the <code>-p</code>
option, for example <code>-p 0.52</code>. This will slow down
convergence, so you will probably also need to increase the number
of shades of colour (the <code>-f</code> option, as discussed above)
in order to make the result look nice.

<li>
To raise an individual root to a power other than 1, put a slash
after that root followed by the power value. For example, an
argument of <code>2+i/1-0.5i</code> specifies a factor of
<code>(x-(2+i))^(1-0.5i)</code>.

<li>
If you want a larger version of an image, just change the picture
size specified in the <code>-s</code> option.

<li>
The program has a number of other options; just type
<code>newton</code> on its own to list them.

</ul>

<p>
Here are some pre-generated larger versions of the above images:

<p>
<a href="bigsimple.png">[bigsimple]</a>
<a href="bigzoomed.png">[bigzoomed]</a>
<a href="bigrainbow.png">[bigrainbow]</a>
<a href="bigrainbowz1.png">[bigrainbowz1]</a>
<a href="bigrainbowz2.png">[bigrainbowz2]</a>

<h2>Recognition</h2>

<p>
Newton-Raphson fractals are not a new idea of mine, although most
other pages I've seen don't go so far into the maths. Here are a few
such pages; you can probably find more by googling for
"Newton-Raphson fractal".

<ul>
<li>
<a href="http://pag.lcs.mit.edu/~adonovan/hacks/newton.html">Alan
Donovan</a> was the person who introduced me to these fractals in
the first place.
<li>
<a href="http://www.tom.womack.net/projects/newton-sets.html">Tom
Womack</a> also thought of animating them, although his page
unfortunately doesn't include any actual movie files.
<li>
<a href="http://astronomy.swin.edu.au/~pbourke/fractals/newtonraphson/">Paul
Bourke</a> has some particularly well-chosen and pretty images.
<li>
<a href="http://www.krofchok.com/fractals/">Bryan Krofchok</a> has a
more varied - and more colourful - gallery.
<li>
<a href="http://mathworld.wolfram.com/NewtonsMethod.html">MathWorld</a>'s
page on the Newton-Raphson method itself mentions its fractal
property, and has some small examples and further references.
</ul>

<hr>
(comments to <a href="mailto:&#97;&#110;&#97;&#107;&#105;&#110;&#64;&#112;&#111;&#98;&#111;&#120;&#46;&#99;&#111;&#109;">&#97;&#110;&#97;&#107;&#105;&#110;&#64;&#112;&#111;&#98;&#111;&#120;&#46;&#99;&#111;&#109;</a>)
<br>
(thanks to
<a href="http://www.chiark.greenend.org.uk/">chiark</a>
for hosting this page)
<br>
(last modified on <!--LASTMOD-->[insert date here]<!--END-->)
</body>
</html>
